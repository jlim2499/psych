<div id="div_notes"><link href="style2.css" rel="stylesheet"/><br/><h4 id="notetitle" style="color: #1a9cce;">Stats (reliability)<span id="note_star_span"></span></h4><br/><div id="notecontent">Reliability refers to the extent to which an experiment, test, or any measurement procedure yields consistent results upon repeated trials. In psychiatry and psychology, reliability is crucial for ensuring that assessments, questionnaires, or diagnostic tools provide stable and dependable results over time, thus contributing to valid, evidence-based clinical practice.<br/><br/><div class="table-responsive"><table class="tlarge table table-striped table-bordered" data-role="table" id="tableid1"><thead><tr><th>Reliability subtype</th><th>Description</th></tr></thead><tbody><tr><td>Test-retest reliability</td><td>Test-retest reliability assesses the stability of a measure over time. The same test is administered to the same group at two different points in time, and the scores are compared. High test-retest reliability indicates that the measure produces consistent results across time intervals. For instance, an anxiety questionnaire should yield similar scores if administered to the same patient over short, stable periods.</td></tr><tr><td>Inter-rater reliability</td><td>Inter-rater reliability measures the consistency of test scores between different raters or observers. It is particularly relevant in clinical settings where different clinicians might assess the same patient. For example, a high inter-rater reliability in a structured psychiatric interview indicates that different clinicians are likely to arrive at similar diagnoses for the same patient.</td></tr><tr><td>Internal consistency reliability</td><td>Internal consistency reliability assesses how consistently items within a test measure the same construct. It is often evaluated using statistical measures like Cronbach's alpha. For example, in a depression inventory, items assessing sadness, fatigue, and hopelessness should all correlate well with each other if they are truly measuring the same underlying construct of depression.</td></tr><tr><td>Parallel-forms reliability</td><td>Parallel-forms reliability is determined by administering two equivalent forms of a test to the same group. If both forms yield similar results, the test is considered reliable. This is useful for assessing constructs that might be influenced by practice effects, such as cognitive function, where exposure to the same items may lead to improved scores on retesting.</td></tr><tr><td>Split-half reliability</td><td>Split-half reliability assesses the consistency of test scores by dividing the test into two halves (e.g., odd and even items) and comparing scores on each half. High split-half reliability indicates that the test measures the construct consistently throughout its items. This method is particularly useful in long assessments, where item consistency can be confirmed without requiring repeated administrations.</td></tr></tbody></table></div><br/>Factors affecting reliability:<br/><br/>Measurement error: Inconsistent results can arise from errors in measurement, such as poorly calibrated tools or unclear test instructions. Measurement error reduces reliability by introducing variability unrelated to the construct being measured.<br/><br/>Sample variability: Highly heterogeneous samples may show lower reliability due to individual differences affecting scores, while highly homogeneous samples may exaggerate reliability as they yield similar responses.<br/><br/>Testing conditions: Changes in testing conditions, such as room environment, time of day, or examiner behaviour, can affect reliability. Standardised testing conditions are essential to maintain reliability in psychiatric and psychological assessments.<br/><br/>Practice effects: For assessments where familiarity with the test content can influence results, repeated administration may lead to practice effects that artificially inflate reliability. This is particularly relevant for cognitive and neuropsychological testing.</div></div>