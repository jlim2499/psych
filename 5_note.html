<div id="div_notes"><link href="style2.css" rel="stylesheet"/><br/><h4 id="notetitle" style="color: #1a9cce;">Stats (hypothesis testing)<span id="note_star_span"></span></h4><br/><div id="notecontent">Generally speaking, it is not possible to investigate hypotheses on entire populations. As such, one usually takes samples and uses these to make estimates about the population from which they are drawn. This leads to uncertainty as there is no guarantee that the sample taken will be truly representative of the population. Such samples therefore involve potential for error.<br/><br/>Statistical hypothesis testing is the procedure used to help decide if claims from samples to populations can be made and with what certainty.<br/><br/><b>Null and alternative hypothesis</b><br/><br/>Statistics can appear back to front. Instead of proving a claim you generally try to disprove the opposite.<br/><br/>For example, imagine that you wish to prove that drug A lowers serum cholesterol. Rather than setting out to prove this you instead seek to try and establish the probability that it does not lead to any real effect on serum cholesterol.<br/><br/>This claim, that there is no real difference is called the null hypothesis (usually indicated by Ho). Imagine a study where the difference found between two groups (one given the drug and one a placebo) was 1mmol/L. Is this a real difference due to the drug or simply a difference due to random variation? The null hypothesis suggests that the difference is due o random variation.<br/><br/>The alternative hypothesis (Indicated by H1 or Ha) is the opposite of the null hypothesis. This is sometimes referred to as the research hypothesis. This suggests that any difference is due to some non-random chance i.e. the difference is real.<br/><br/>The alternative hypothesis can be one or two-tailed.<br/><br/>If the alternative hypothesis simply seeks to establish a difference then it is two-tailed. For example you may wish to investigate if the mean weight of people has changed over the past 10 years. In this case you are not suggesting that people have increased or decreased in weight but simply changed.<br/><br/>If you are seeking to investigate a change in something in one direction then this is referred to as one-tailed. For example, you may suspect that clozapine therapy increases peoples' weights. You are not just investigating if clozapine changes peoples' weight but are also suggesting that it changes it in one particular direction (that is up in this case).<br/><br/>Here is an example to clarify these terms.  You wonder if those people taking paper 1 of the MRCPsych for the second time get higher marks.  The null hypothesis would be, those Candidates taking paper 1 for the second time get the same result as those taking it for the first time (i.e. there is no difference).  A one-tailed alternative hypothesis might be, those taking paper 1 for the second time get higher marks than those taking it for the first time.  A two-tailed alternative hypothesis might be, those taking the exam for the second time get different results to those taking it for the first time.<br/><br/><b>Type I and II error</b><br/><br/>Two types of errors may occur when testing the null hypothesis<br/><br/><ul><li>Type I: the null hypothesis is rejected when it is true (finding a difference that didn't exist), this is also called a false positive. This is determined against a pre-set significance level (termed alpha). As the significance level is determined in advance the chance of making a type I error is not affected by sample size.</li></ul><br/><ul><li>Type II: the null hypothesis is accepted when it is false (failing to find a difference that really existed), this is also called a false negative. The probability of making a type II error is termed beta. It is determined by both sample size and alpha.</li></ul><br/><div class="table-responsive"><table class="tlarge table table-striped table-bordered" data-role="table" id="tableid1"><thead><tr><th></th><th>Ho False<br/>(there is a difference / change has occurred)</th><th>Ho True<br/>(there is no difference / no change has occurred)</th></tr></thead><tbody><tr><td>Ho rejected by data analysis<br/>(study concludes there is a difference / change has occurred)</td><td>Correct decision<br/>(1 - β = Power of the test)</td><td>Type I error (alpha)</td></tr><tr><td>Data analysis failed to reject Ho <br/>(study concludes there is a difference / change has occurred)</td><td>Type II error (β)</td><td>Correct decision</td></tr></tbody></table></div><br/>The type I and II errors are easily confused. For those who are visual learners, the graphic may serve to illustrate the difference in a memorable way.<br/><br/><center><table><tbody><tr><td align="left" colspan="2"><img class="ajaximage" src="https://d3a7z3xcyhqwt4.cloudfront.net/images_MRCPsychmentor/mma098.png"/></td></tr><tr><td align="left" valign="top"></td><td align="right"></td></tr></tbody></table></center><br/>Another way to remember the difference is:<br/><br/><ul><li>A Type I error is a false POSITIVE; and P has a single vertical line.</li><li>A Type II error is a false NEGATIVE; and N has two vertical lines.</li></ul><br/>The power of a study is the probability of (correctly) rejecting the null hypothesis when it is false<br/><br/><ul><li>Power = 1 - the probability of a type II error</li><li>Power can be increased by increasing the sample size</li></ul><br/><b>P-values</b><br/><br/>P-values provide information on statistical significance. They help us decide if study results have occurred due to chance.<br/><br/>P-values tell us about chance but not other things such as bias. <br/><br/>The p-value is the probability of obtaining a result that is as large or larger when in reality there is no difference between two groups. <br/><br/>Alternatively you can understand this as the probability of rejecting the null when it is true.<br/><br/>The p-value can take on any number from 0 to 1 (as it is a probability). A high p-value would indicate that there is a high chance that an observed difference is due to chance and a low p-value that there is a low probability that the observation is due to chance.<br/><br/>Before finding a p-value you must decide on a cut-off level. This relates to how sure you need to be (it's just like in the courts where you have a 'beyond reasonable doubt' and 'balances of probabilities'). The cut-off is called the significance level (aka alpha level). Typically, this cut-off is set at 0.05 (this is the 5% level where there is a less than 1 in 20 chance of being wrong).<br/><br/>If the p-value is found to be less than the cut-off then you reject the null hypothesis. <br/><br/>If the p-value is greater or equal to the cut-off then you fail to reject the null.<br/><br/>Note that the p-value can indicate that a result is statistically significant but not that the result is clinically significant. The result may be so small that it is not clinically meaningful.<br/><br/><b>Bonferroni correction</b><br/><br/>The Bonferroni correction is a statistical method used to address the issue of multiple comparisons in hypothesis testing. When performing multiple tests on the same dataset, the likelihood of committing a Type I error (false positive) increases with the number of comparisons made. The Bonferroni correction helps control this by adjusting the significance level (alpha) for each individual test.<br/><br/>The correction works by dividing the overall significance level (typically 0.05) by the number of tests being conducted. For example, if five tests are performed, the new significance level for each test would be 0.05/5 = 0.01.<br/><br/>By using the Bonferroni correction, the overall probability of committing a Type I error across all tests remains at the desired alpha level, reducing the chance of false positives.<br/><br/>It is important to note that while the Bonferroni correction reduces the risk of Type I errors, it also makes it harder to detect true effects, increasing the likelihood of Type II errors (false negatives). This trade-off should be carefully considered in studies with many comparisons or smaller sample sizes.<br/><br/>Clinically, the Bonferroni correction is particularly important in psychiatric research where multiple outcomes (e.g., different symptom scales) are often evaluated. Without this adjustment, there is a higher chance of erroneously concluding that a treatment effect exists when it does not.</div></div>