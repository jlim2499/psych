<div id="div_question"><link href="style1.css" rel="stylesheet"/>Statistical power is defined as the probability of:<br/><div><br/><span style="background-color: #A5FF7F; padding: 11px 22px; border-radius: 4px; border-left: 5px solid green;">Detecting a true effect when one exists</span></div><br/><div class="alert alert-success" role="alert">Power is the probability of correctly rejecting the null hypothesis when it is false (1 - β), meaning it detects a real effect and avoids a Type II error. A power of 80% (0.80) is typically considered the minimum acceptable level in clinical research.</div>The correct answer is <b>Detecting a true effect when one exists</b>. Statistical power is defined as the probability of correctly rejecting the null hypothesis when it is false, meaning it is the likelihood of identifying a true effect or difference in a study. Power is typically expressed as 1 - β, where β is the probability of a Type II error (failing to detect a true effect).<br/><br/><b>Rejecting the null hypothesis when it is true</b>: This describes a Type I error (false positive), not statistical power. Type I errors are controlled by the significance level (alpha), not power.<br/><br/><b>Accepting the null hypothesis when it is false</b>: This describes a Type II error (false negative), which is inversely related to power. As power increases, the likelihood of making a Type II error decreases.<br/><br/><b>Committing a Type I error</b>: This is unrelated to statistical power. Type I errors are determined by the alpha level, which sets the threshold for rejecting the null hypothesis.<br/><br/><b>The effect size divided by the sample size</b>: While effect size and sample size both influence power, this is not a definition of statistical power. Power is the probability of detecting a true effect, not a direct function of the ratio between effect size and sample size.<br/><br/>This question is clinically relevant because understanding statistical power is essential for designing studies with adequate sample sizes to detect clinically meaningful differences. Underpowered studies may miss true effects, potentially delaying the adoption of effective treatments or interventions in psychiatric practice.<br/></div>