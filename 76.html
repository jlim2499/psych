<div id="div_question"><link href="style1.css" rel="stylesheet"/>A research study aims to assess the agreement between two psychiatrists diagnosing bipolar disorder using a new diagnostic tool. Which statistical measure is most appropriate for quantifying the degree of agreement beyond chance?<br/><div><br/><span style="background-color: #A5FF7F; padding: 11px 22px; border-radius: 4px; border-left: 5px solid green;">Cohen's Kappa coefficient</span></div><br/><div class="alert alert-success" role="alert">Inter-rater agreement / reliability is measured by Kappa.</div><i><b>Previous Exam Question</b></i><br/><br/>The correct answer is <b>Cohen's Kappa coefficient</b>. Cohen's Kappa measures the agreement between two raters or assessors, accounting for the agreement that occurs by chance. It is particularly useful when assessing categorical data, such as diagnostic categories (e.g., bipolar disorder vs other diagnoses). This measure is highly relevant in clinical contexts where consistency between practitioners is critical, ensuring that patients receive reliable and standardised care. For example, if two psychiatrists are diagnosing bipolar disorder, Cohen's Kappa would quantify how consistent their diagnoses are beyond random agreement, thus improving diagnostic accuracy.<br/><br/><b>Pearson correlation coefficient</b>: This measures the strength and direction of a linear relationship between two continuous variables. It is not suitable for categorical data such as diagnostic categories and does not account for chance agreement. Using Pearson's correlation in this context would not accurately capture agreement between raters.<br/><br/><b>Spearman's rank correlation coefficient</b>: This measures the monotonic relationship between two ranked (ordinal) variables. It is not appropriate for nominal categories, such as diagnostic classifications, and does not account for agreement beyond chance.<br/><br/><b>Intraclass correlation coefficient</b>: This is used for assessing the reliability of measurements or ratings when the data are continuous or ordinal. It is not suitable for nominal data such as diagnostic categories.<br/><br/><b>Chi-square test</b>: This evaluates the association or independence between two categorical variables but does not quantify agreement between raters or account for chance agreement. It would not provide the nuanced understanding of inter-rater reliability that Cohen's Kappa offers.<br/><br/></div>